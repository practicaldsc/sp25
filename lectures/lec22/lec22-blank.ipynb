{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568ab87",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to get everything set up.\n",
    "from lec_utils import *\n",
    "import lec22_util as util\n",
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = diabetes[(diabetes['Glucose'] > 0) & (diabetes['BMI'] > 0)]\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eff536",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 22\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "### EECS 398: Practical Data Science, Spring 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> â€¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/sp25\">github.com/practicaldsc/sp25</a> â€¢ ðŸ“£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/78535/discussion/6647877) </small>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85a19b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda ðŸ“†\n",
    "\n",
    "- Predicting probabilities.\n",
    "- Cross-entropy loss.\n",
    "- From probabilities to decisions.\n",
    "\n",
    "Check out this [Decision Boundary Visualizer](https://ml-visualizer.herokuapp.com/), which allows you to visualize decision boundaries for different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057404d9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63b481",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting probabilities ðŸŽ²\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34df328",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"imgs/needle.png\" width=900>\n",
    "<br>\n",
    "The New York Times maintained <a href=\"https://www.nytimes.com/interactive/2024/11/05/us/elections/results-president-forecast-needle.html\">needles</a><br>that displayed the probabilities of various outcomes in the election.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc4836",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation: Predicting probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768adbf4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Often, we're interested in predicting the **probability** of an event occurring, given some other information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83500d58",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Given that the score at the start of the second half is Michigan 23-Northwestern 15,<br>what's the probability that Michigan wins?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea4770",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Here's a picture of an animal. What's the probability it's of a dog? Cat? Hamster? Zebra?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846b3d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>What's the probability that it rains on campus tomorrow?<br><small>In the context of weather apps, this is a nuanced question; <a href=\"https://xkcd.com/1985\">here's a meme about it</a>.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c63e8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we're able to predict the probability of an event, we can **classify** the event by using a threshold.<br><small>For example, if we predict there's a 70% chance of Michigan winning, we could predict that Michigan will win. Here, we implicitly used a threshold of 50%.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b5bea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The two classification techniques we've seen so far â€“ $k$-nearest neighbors and decision trees â€“ **don't** directly use probabilities in their decision-making process.<br><small>But sometimes it's helpful to model uncertainty and to be able to state a level of confidence along with a prediction!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5096d223",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Predicting diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a4f554",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As before, <span style='color: orange'><b>class 0 (orange) is \"no diabetes\"</b></span> and <span style='color: blue'><b>class 1 (blue) is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf20510",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "util.create_base_scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa6897",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try to predict whether or not a patient has diabetes (`'Outcome'`) given just their `'Glucose'` level.<br><small>Last class, we used both `'Glucose'` and `'BMI'`; we'll start with just one feature for now.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d7f92",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396c1f2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that as a patient's `'Glucose'` value increases, the **chances they have diabetes** also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9205370",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can we model this probability directly, as a function of `'Glucose'`?<br>In other words, can we find some $f$ such that:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = f(\\text{Glucose}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38f65b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An attempt to predict probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9897306",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try and fit a simple linear model to the data from the previous slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a251ad0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_linear_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b94667",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <span style=\"color:#097054\"><b>simple linear model</b></span> above predicts values greater than 1 and less than 0! This means we can't interpret the outputs as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ade9c3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We could, technically, **clip** the outputs of the linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54a448",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_linear_model_clipped(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421c5a7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bins and proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca53f9f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another approach we could try is to:\n",
    "    - Place `'Glucose'` values into **bins**, e.g. 50 to 55, 55 to 60, 60 to 65, etc.\n",
    "    - Within each bin, compute the proportion of patients in the training set who had diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201fe8c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the source code in lec22_util.py to see how we did this!\n",
    "# We've hidden a lot of the plotting code in the notebook to make it cleaner.\n",
    "util.make_prop_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e00d75",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, the point near a `'Glucose'` value of 100 has a $y$-axis value of ~0.25. This means that about 25\\% of patients with a `'Glucose'` value near 100 had diabetes in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e54fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, if a new person comes along with a `'Glucose'` value near 100, we'd predict there's a 25\\% chance they have diabetes (so they likely do not)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b453b2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Notice that the points form an S-shaped curve!**<br><small>Can we incorporate this S-shaped curve in how we predict probabilities?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a1fcf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55939a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **logistic function** resembles an $S$-shape.\n",
    "\n",
    "    $$\\sigma(t) = \\frac{1}{1 + e^{-t}} = \\frac{1}{1 + \\text{exp}(-t)}$$\n",
    "    \n",
    "    <br><small>The logistic function is an example of a <b>sigmoid function</b>, which is the general term for an S-shaped function. Sometimes, we use the terms \"logistic function\" and \"sigmoid function\" interchangeably.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e93aef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Below, we'll look at the shape of $y = \\sigma(w_0 + w_1 x)$ for different values of $w_0$ and $w_1$.\n",
    "    - $w_0$ controls the position of the curve on the $x$-axis.\n",
    "    - $w_1$ controls the \"steepness\" of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a775146",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_three_sigmoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1452c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice that $0 < \\sigma(t) < 1$, for all $t$, which means **we can interpret the outputs of $\\sigma(t)$ as probabilities**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81117770",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below, interact with the sliders to change the values of $w_0$ and $w_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2aa589",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(util.plot_sigmoid, w0=(-15, 15), w1=(-3, 3, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128bba3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c642d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic **regression** is a linear **classification** technique that builds upon linear regression.<br><small>It is **not** called logistic**al** regression!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aed0dc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It models **the probability of belonging to class 1, given a feature vector**:\n",
    "    \n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (\\underbrace{w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}}_{\\text{linear regression model}}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e1f47",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the existence of coefficients, $w_0, w_1, ... w_d$, that we need to learn from the data, tells us that logistic regression is a **parametric** method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52f2a6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68267582",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26197bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a `LogisticRegression` classifier. Specifically, this means we're asking `sklearn` to learn the optimal parameters $w_0^*$ and $w_1^*$ in:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma \\left( w_0 + w_1 \\cdot \\text{Glucose}_i \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573c0ca",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression()\n",
    "model_logistic.fit(X_train[['Glucose']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb21cee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We get a test accuracy that's roughly in line with the test accuracies of the two models we saw last class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b4fe3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.score(X_test[['Glucose']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6cd81",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does our fit model **look like**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891f503",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing a fit logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2772434",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The values of $w_0^*$ and $w_1^*$ `sklearn` found are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b8ad0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.intercept_[0], model_logistic.coef_[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23269e61",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, our fit model is:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma(-5.59 + 0.04 \\cdot \\text{Glucose}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69983d77",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb503803",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, if a patient has a `'Glucose'` level of 150, the model's predicted probability that they have diabetes is:\n",
    "\n",
    "$$\\sigma(-5.59 + 0.04 \\cdot 150) \\approx \\sigma(0.41) \\approx 0.601$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a526c7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict_proba([[150]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd54a8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><big>How did <code>sklearn</code> find $w_0^*$ and $w_1^*$?<br>What <b>loss function</b> did it use?</big></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e2d46",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-entropy loss\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c91af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The modeling recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22654d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To train a **parametric model**, we always follow the same three steps.\n",
    "<br><small>$k$-Nearest Neighbors and decision trees didn't quite follow the same process.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62225124",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Choose a model.\n",
    "\n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de1379",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Choose a loss function.\n",
    "\n",
    "<center>???</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac2af06",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Minimize average loss to find optimal model parameters.<br><small>As we've now seen, average loss could also be regularized!</small>\n",
    "\n",
    "<center>???</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06750648",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attempting to use squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd16ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our default loss function has always been squared loss, so we could try and use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bbcc99",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690eb21d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unfortunately, there's no closed form solution for $\\vec{w}^*$, so we'll need to use gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e5ab6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Before doing so, let's visualize the **loss surface** in the case of our \"simple\" logistic model:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma(w_0 + w_1 \\cdot \\text{Glucose}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8925904",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, we'll visualize:\n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma(w_0 + w_1 \\underbrace{x_i}_{\\text{Glucose}_i} ) \\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c508f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_mse_surface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fadc4f2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **What do you notice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e660ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean squared error doesn't work well with logistic regression! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b769daa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The following function is **not** convex:\n",
    "\n",
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e692d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are two flat \"valleys\" with gradients near 0, where gradient descent could get trapped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a88e6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Additionally, squared loss doesn't penalize bad predictions nearly enough. The largest possible value of:\n",
    "\n",
    "    $$\\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$\n",
    "\n",
    "    is 1, since both $y_i$ and $\\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$ are **bounded** between 0 and 1, and $(1 - 0)^2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e706f42",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose $y_i = 1$. Then, the graph of the squared loss of the prediction $p_i$ is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39772c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_squared_loss_individual()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a924564",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Predicted $p_i$ values near 0 are really bad, since $y_i = 1$, but the loss for $p_i = 0$ is not very high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf82ac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems like we need a loss function that more **steeply penalizes incorrect probability predictions** â€“ and hopefully, one that is convex for the logistic regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74115f49",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc9b82",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A common loss function in this setting is **log loss**, i.e. **cross-entropy loss**.<br><small>The term \"entropy\" comes from information theory. Watch [**this short video**](https://www.youtube.com/watch?v=ErfnhcEV1O8) for more details.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383e484",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can define the cross-entropy loss function piecewise. If $y_i$ is an observed value and $p_i$ is a predicted **probability**, then: \n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = \\begin{cases} - \\log(p_i) & \\text{if $y_i = 1$} \\\\ -\\log(1 - p_i) & \\text{if $y_i = 0$} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3f13a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that in the two cases â€“ $y_i = 1$ and $y_i = 0$ â€“ the cross-entropy loss function resembles squared loss, but is unbounded when the predicted probabilities $p_i$ are far from $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace669b7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ce_loss_individual_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04fe5c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ce_loss_individual_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10470670",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A non-piecewise definition of cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4af6d5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can define the cross-entropy loss function piecewise. If $y_i$ is an observed value and $p_i$ is a predicted **probability**, then: \n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = \\begin{cases} - \\log(p_i) & \\text{if $y_i = 1$} \\\\ -\\log(1 - p_i) & \\text{if $y_i = 0$} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54680fa6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An equivalent formulation of $L_\\text{ce}$ that isn't piecewise is:\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03114d02",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This formulation is easier to work with algebraically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e638cb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Average cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc91b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Cross-entropy loss** for an observed value $y_i$ and predicted **probability** $p_i = P(y_i = 1 | \\vec{x}_i) = \\sigma \\left(\\vec w \\cdot \\text{Aug}(\\vec x_i) \\right)$ is:\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49900c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find $\\vec{w}^*$, then, we minimize **average cross-entropy loss**:\n",
    "\n",
    "\\begin{align*}R_\\text{ce}(\\vec{w}) &= - \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right) \\\\ &= - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right]\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d94796",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cross-entropy loss is the default loss function used to find optimal parameters in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c2ec7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There's still no closed-form solution for $\\vec{w}^* = \\underset{\\vec{w}}{\\text{argmin}} \\: R_\\text{ce}(\\vec{w})$, so we'll need to use gradient descent, or some other numerical method.<br><small>But don't worry â€“ we'll leave this to `sklearn`!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d30d42",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Fortunately, average cross-entropy loss is convex, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef920929",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_ce_surface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a70495",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- And, it can be regularized!<br><small>By default, `sklearn` applies regularization when performing logistic regression.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee4e27",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_ce_surface(X_train, y_train, reg_lambda=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5f809",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The modeling recipe, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b7806",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Choose a model.\n",
    "\n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae21c53",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Choose a loss function.\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$\n",
    "\n",
    "$$\\text{where} \\: p_i = P(y_i = 1 | \\vec{x}_i) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb0694",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Minimize average loss to find optimal model parameters.<br><small>As we've now seen, average loss could also be regularized!</small>\n",
    "\n",
    "    \\begin{align*}R_\\text{ce}(\\vec{w}) &= - \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right) \\\\ &= - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right]\\end{align*}\n",
    "\n",
    "    <br>\n",
    "\n",
    "    The actual minimization here is done using numerical methods, through `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d03aa2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326a932",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `LogisticRegression` class in `sklearn` has a lot of hidden, default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194d14d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b49599",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It performs $L_2$ regularization (\"ridge logistic regression\") **by default**. The hyperparameter for regularization strength, $C$, is the **inverse** of $\\lambda$; by default, it sets $C = 1$.\n",
    "\n",
    "$$C = \\frac{1}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d600b2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, for a given value of $C$, it minimizes:\n",
    "\n",
    "$$R_\\text{ce-reg}(\\vec{w}) = - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right] + \\frac{1}{C} \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121401ba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It also specifies `solver='lbfgs'`, i.e. it doesn't use gradient descent per-se, but another more sophisticated numerical method.<br><small>Read more about LBFGS [here](https://en.wikipedia.org/wiki/Limited-memory_BFGS).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708a679",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">By default, logistic regression in scikit-learn runs w L2 regularization on and defaulting to magic number C=1.0. How many millions of ML/stats/data-mining papers have been written by authors who didn&#39;t report (&amp; honestly didn&#39;t think they were) using regularization?</p>&mdash; Zachary Lipton (@zacharylipton) <a href=\"https://twitter.com/zacharylipton/status/1167298276686589953?ref_src=twsrc%5Etfw\">August 30, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64087819",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9efe77",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From probabilities to decisions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e4439",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting probabilities vs. predicting classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f967b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2c5de",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ðŸ¤” **Question**: Suppose our logistic regression model predicts the probability that someone has diabetes is 0.75. What do we predict â€“ diabetes or no diabetes? What if the predicted probability is 0.3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50953c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ðŸ™‹ **Answer**: We have to pick a threshold (for example, 0.5)!\n",
    "    - If the predicted probability is above the threshold, we predict diabetes (1).\n",
    "    - Otherwise, we predict no diabetes (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff28d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting probabilities vs. predicting classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebf034",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, the `predict` method of a fit `LogisticRegression` model predicts a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4978f9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9f207",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, logistic regression is designed to predict **probabilities**. We can access these predicted probabilities using the `predict_proba` method, as we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877fc803",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict_proba(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ef01e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The above is telling us that the model thinks this person has:\n",
    "    - A 40% chance of belonging to class 0 (no diabetes).\n",
    "    - A 60% chance of belonging to class 1 (diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a7467",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, it uses a threshold of 0.5, i.e. it predicts the larger probability.<br>As we'll discuss next class, this may not be what we want!<br><small>Unfortunately, `sklearn` doesn't let us change the threshold ourselves. If we want a different threshold, we need to manually implement it using the results of `predict_proba`.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6691d85",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thresholding probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463efaf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we did with other classifiers, we can visualize the **decision boundary** of a fit logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6151a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we pick a threshold of $T$, any patient with a `'Glucose'` value such that: \n",
    "\n",
    "    $$\\sigma(w_0^* + w_1^* \\cdot \\text{Glucose}_i) \\geq T$$ \n",
    "\n",
    "    is classified as having diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763d544",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, if $T = 0.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec374d0a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_y_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d69671",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we set $T = 0.5$, then patients with `'Glucose'` values above $\\approx$ 140 are classified as having diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cb85b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_x_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5851cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How do we find the exact $x$-axis position of the <span style=\"color:purple\">decision boundary</span> above?**<br><small>If we can, then we'd be able to predict whether someone has diabetes just by looking at their `'Glucose'` value.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e20228",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision boundaries for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64a4c4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In our single feature model that predicts `'Outcome'` given just `'Glucose'`, our predicted probabilities are of the form:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1dbdbc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we fix a threshold, $T$. Then, our <b><span style=\"color:purple\">decision boundary</span></b> is of the form:\n",
    "\n",
    "$$\\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_T \\right) = T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfabf8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we can invert $\\sigma(t)$, then we can re-arrange the above to solve for the `'Glucose'` value at the threshold:\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\sigma^{-1}(T) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc91cb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: If $p = \\sigma(t)$, then $\\sigma^{-1}({p}) = \\log \\left( \\frac{p}{1-p} \\right)$ is the inverse of $\\sigma(t)$.<br><small>$\\sigma^{-1}(p)$ is called the **logit** function.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d08d50",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9cea3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose an event occurs with probability $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230dc88",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **odds** of that event are:\n",
    "\n",
    "$$\\text{odds}(p) = \\frac{p}{1-p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f70c4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For instance, if there's a $p = \\frac{3}{4}$ chance that Michigan wins this week, then the **odds** that Michigan wins this week are:\n",
    "\n",
    "    $$\\text{odds} \\left( \\frac{3}{4} \\right) = \\frac{\\frac{3}{4}}{\\frac{1}{4}} = 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38332eee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Interpretation: it's 3 times more likely that Michigan wins than loses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c2f3d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **We can interpret $\\sigma^{-1}(p) = \\log \\left( \\frac{p}{1-p} \\right)$ as the \"log odds\" of $p$!**<br><small>See the reference slides for more details.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa5e30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving for the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f48f5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Previously, we said that if we pick a threshold $T$, then:\n",
    "\n",
    "$$\\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_T \\right) = T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011e210",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We re-arranged this for the `'Glucose'` value on the threshold, $\\text{Glucose}_T$:\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\sigma^{-1}(T) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53799521",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using the fact that $\\sigma^{-1}(T) = \\log \\left( \\frac{T}{1 - T} \\right)$ gives us a closed-form formula for $\\text{Glucose}_T$!\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\log \\left( \\frac{T}{1-T} \\right) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6825ee6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **This explains why $\\text{Glucose} \\geq 140$ is the <span style=\"color:purple\">decision boundary</span> below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e7c34",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_star = model_logistic.intercept_[0]\n",
    "w1_star = model_logistic.coef_[0][0]\n",
    "T = 0.5\n",
    "glucose_threshold = (np.log(T / (1 - T)) - w0_star) / w1_star\n",
    "glucose_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c4cad0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_x_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc62645",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The decision boundary in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74488f3d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The decision boundary on the previous slide is:\n",
    "\n",
    "$$\\text{Glucose}_T \\geq 140$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a9347",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's visualize this in the **feature space**. We are just using $d = 1$ feature, so let's visualize our decision boundary with a 1D plot, i.e. a number line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6344a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_in_1D(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0c4a4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression with multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e092c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, as we did last class, let's use both `'Glucose'` and `'BMI'` to predict diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d60f92",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.create_base_scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c13222",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, our fit model will look like:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i, \\text{BMI}_i) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_i + w_2^* \\cdot \\text{BMI}_i \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3947b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple = LogisticRegression()\n",
    "model_logistic_multiple.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef0d01c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After minimizing mean (regularized!) cross-entropy loss, we find that our fit model is of the form:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i, \\text{BMI}_i) = \\sigma \\left( -7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecaaae5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple.intercept_, model_logistic_multiple.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0828aa1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing a fit logistic regression model with two features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca62f1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, the logistic regression model is trained to predict the probability of <b><span style=\"color:blue\">class 1 (diabetes)</span></b>.\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i, \\text{BMI}_i) = \\sigma \\left( -7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a4557",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The graph below shows the predicted probabilities of <b><span style=\"color:blue\">class 1 (diabetes)</span></b> for different combinations of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ddd497",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic(model_logistic_multiple, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b7cb7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The decision boundary in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbef716",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does the resulting decision boundary look like, in a $d = 2$ dimensional plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7af28",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_logistic_multiple, X_train, y_train, title='Logistic Regression Decision Boundary (T = 0.5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ceb7d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that unlike the decision boundaries for $k$-nearest neighbors and decision trees, this decision boundary is **linear**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd69f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, the decision boundary in the feature space is of the form:\n",
    "\n",
    "$$a \\cdot \\text{Glucose}_i + b \\cdot \\text{BMI}_i + c = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb55af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **In Homework 11 (released later this week), you'll solve for $a$, $b$, and $c$ in a similar example!**<br><small>It involves retracing the steps we followed in the single-feature case.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf1c110",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488c681",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Properties of the logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09eaba3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- The logistic function, $\\sigma(t)$, obeys several interesting properties. \n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63087975",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- It is **symmetric**.\n",
    "\n",
    "$$\\sigma(-t) = 1 - \\sigma(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52a7ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Its **derivative** is conveniently calculated:\n",
    "\n",
    "$$\\frac{d}{dt}\\sigma(t) = \\sigma(t) (1 - \\sigma(t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ca892",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- But, most relevant to us right now, its **inverse** is:\n",
    "\n",
    "$$p = \\sigma(t) \\implies t = \\sigma^{-1}(p) = \\log \\left( \\frac{p}{1-p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d36b6f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Linearity of log odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd020751",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Let $p$ represent our predicted probability.\n",
    "\n",
    "$$p = P(y_i = 1 | \\text{Glucose}_i ) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eca627",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Using the inverse of the logistic function, we have that:\n",
    "\n",
    "$$w_0^* + w_1^* \\cdot \\text{Glucose}_i = \\log \\left( \\frac{p}{1 - p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f0ca6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- On the left, we have a **linear function of $\\text{Glucose}_i$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214bf49",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- On the right, we have the **log of the odds** of $p$.<br><small>We call the \"log of the odds\" the \"log odds\".</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abc9c0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- **Important**: The logistic regression model assumes that **the log of the odds of $P(y_i = 1 | \\vec{x}_i)$ is linear!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce1d722",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Implications of the linearity of log odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb1e344",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Suppose that $w_0^* = -6$ and $w_1^* = 0.05$. Then:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma(-6 + 0.05 \\cdot \\text{Glucose}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196ed79",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- It's hard to interpret the role of the coefficient $0.05$ directly. But, we know that:\n",
    "\n",
    "$$-6 + 0.05 \\cdot \\text{Glucose}_i = \\log \\left( \\frac{p}{1 - p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1421a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Example: Suppose my `'Glucose'` level increases by 1 unit. Then, the predicted log odds that I have diabetes increases by 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9001f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- But, since:\n",
    "\n",
    "$$e^{-6 + 0.05 \\cdot \\text{Glucose}_i} = \\frac{p}{1-p} = \\text{odds}(p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1352d940",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- And:\n",
    "\n",
    "$$e^{-6 + 0.05 \\cdot (\\text{Glucose}_i + 1)} = e^{-6 + 0.05 \\cdot \\text{Glucose}_i} \\cdot e^{0.05}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ae57f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- We can say that **if my `'Glucose'` level increases by 1 unit, then my predicted odds of diabetes increases by a _factor_ of $e^{0.05}$**, or more generally $e^{w_1^*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf266b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- You'll need this interpretation in Homework 11!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915276c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Which expression describes the **odds ratio**, $$\\frac{P(y_i = 1 | \\vec{x}_i)}{P(y_i = 0 | \\vec{x}_i)}$$\n",
    "    \n",
    "in the logistic regression model?\n",
    "    \n",
    "- A. $\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)$\n",
    "- B. $-\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)$\n",
    "- C. $e^{\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)}$\n",
    "- D. $\\sigma(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i))$\n",
    "- E. None of the above.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
