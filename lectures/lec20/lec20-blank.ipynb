{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86677e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec20_util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3e8b1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 20\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "### EECS 398: Practical Data Science, Spring 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/sp25\">github.com/practicaldsc/sp25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/78535/discussion/6647877) </small>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0a4fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Intuition for gradient descent üóª.\n",
    "- When is gradient descent guaranteed to work?\n",
    "- Gradient descent for functions of multiple variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96ce30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/motivation.png\" width=1000><br><small>What we're building towards today.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3567dad",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfcad4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition for gradient descent üóª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac68f8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's go hiking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a39ab6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Suppose you're at the top of a mountain üèîÔ∏è and need to get **to the bottom**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d4812",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Further, suppose it's really cloudy ‚òÅÔ∏è, meaning you can only see a few feet around you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200f005",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- **How** would you get to the bottom?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58ae47",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing arbitrary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc422a6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume $f(w)$ is some **differentiable** function.<br><small>For now, we'll assume $f$ takes in a scalar, $w$, as input and returns a scalar as its output.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f5172",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When tasked with minimizing $f(w)$, our general strategy has been to:<br>\n",
    "    1. Find $\\frac{df}{dw}(w)$, the derivative of $f$.\n",
    "    2. Find the input $w^*$ such that $\\frac{df}{dw}(w^*) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d63b9d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, there are cases where we can find $\\frac{df}{dw}(w)$, but **it is either difficult or impossible to solve $\\frac{df}{dw}(w^*) = 0$**. Then what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7924ce2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f(w) = 5w^4 - w^3 - 5w^2 + 2w - 9$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed3a44",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.draw_f()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2c86a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does the derivative of a function tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35c630",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Goal**: Given a **differentiable** function $f(w)$, find the input $w^*$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d8fd2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does $\\frac{d}{dw} f(w)$ mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ffc2d1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(util.show_tangent, w0=(-1.5, 1.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76641c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Searching for the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666f448",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given an initial _guess_ for a value of $w$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b7a42",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"imgs/positive-slope.png\" width=500>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e87151",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the <span style=\"color:red\">**slope of the tangent line at $f(w)$**</span> is **positive** üìà:\n",
    "    - Increasing $w$ **increases** $f$.\n",
    "    - This means the minimum must be to the **left** of the point $(w, f(w))$.\n",
    "    - Solution: **Decrease** $w$ ‚¨áÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb90e4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steeper the slope is, the further we must be from the minimum ‚Äì so, the steeper the slope, the quicker we should decrease $w$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42219eea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Searching for the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bd698",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given an initial _guess_ for a value of $w$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a931c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"imgs/negative-slope.png\" width=500>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b05c093",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the <span style=\"color:red\">**slope of the tangent line at $f(w)$**</span> is **negative** üìâ:\n",
    "    - Increasing $w$ **decreases** $f$.\n",
    "    - This means the minimum must be to the **right** of the point $(w, f(w))$.\n",
    "    - Solution: **Increase** $w$ ‚¨ÜÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d15551",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steeper the slope is, the further we must be from the minimum ‚Äì so, the steeper the slope, the quicker we should increase $w$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93352d6c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f430d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize a **differentiable** function $f$:\n",
    "    1. Pick a positive number, $\\alpha$. This number is called the **learning rate**, or **step size**.<br><small>Think of $\\alpha$ as a hyperparameter of the minimization process.</small>\n",
    "    2. Pick an **initial guess**, $w^{(0)}$.\n",
    "    3. Then, repeatedly update your guess using the **update rule**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8743c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f2922",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Repeat this process until **convergence** ‚Äì that is, when the difference between $w^{(t)}$ and $w^{(t+1)}$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1404682",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This procedure is called **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec09e7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f94f59",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is a numerical method for finding the input to a function $f$ that minimizes the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5155c1f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is called **gradient descent** because the gradient is the extension of the derivative to functions of multiple variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038a376",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **numerical method** is a technique for approximating the solution to a mathematical problem, often by using the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e1e88",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is **widely used** in machine learning, to train models from linear regression to neural networks and transformers (including ChatGPT)!<br><small>In machine learning, we use gradient descent to minimize empirical risk when we can't minimize it by hand, which is true in most, more sophisticated cases.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02ec6c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee9ab8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, we typically don't implement gradient descent ourselves ‚Äì we rely on existing implementations of it. But, we'll implement it here ourselves to understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa46b10",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's start with an initial guess $w^{(0)} = 0$ and a learning rate $\\alpha = 0.01$.\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10670d26",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd77e23",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We see that pretty quickly, $w^{(t)}$ converges to $-0.727$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e13d3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090813cc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee6bb4f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 1.1, \\alpha = 0.01$\n",
    "\n",
    "What if we start with a different initial guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcca9d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=1.1, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b45fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 0.1$\n",
    "\n",
    "What if we use a different learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42b548",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d74e7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 1$\n",
    "\n",
    "Some learning rates are so large that the values of $w$ explode towards infinity! Watch what happens when we use a learning rate of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982565d1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w = 0\n",
    "for t in range(50):\n",
    "    print(round(w, 4), round(util.f(w), 4))\n",
    "    w = w - 1 * util.df(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0aeb3e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lingering questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe68ef7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When is gradient descent _guaranteed_ to converge to a global minimum? What kinds of functions work well with gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9d0a8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we choose a step size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a87e0e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we use gradient descent to minimize functions of multiple variables, e.g.:\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "\n",
    "<center><small>This is a function of $d+1$ variables: $w_0, w_1, ..., w_d$.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f20e06",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: Why **can't** we use gradient descent to find $\\vec{w}_\\text{LASSO}^*$?\n",
    "\n",
    "$$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_d|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c01b20",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When is gradient descent guaranteed to work?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db7c19",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What makes a function convex?\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 50%; float: left\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"imgs/convex.png\">\n",
    "<center>A <b>convex</b> function ‚úÖ.</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 50%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"imgs/non-convex.png\">\n",
    "<center>A <b>non-convex</b> function ‚ùå.</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10315fd6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intuitive definition of convexity\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 50%; float: left\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/convex.png\" width=70%></center>\n",
    "<center>A <b>convex</b> function ‚úÖ.</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 50%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/non-convex.png\" width=70%></center>\n",
    "<center>A <b>non-convex</b> function ‚ùå.</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1053f67",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A function $f$ is **convex** if, for **every** $a, b$ in the domain of $f$, the line segment between:\n",
    "\n",
    "  $$(a, f(a)) \\text{ and } (b, f(b))$$\n",
    "\n",
    "  does not go below the plot of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259c4dd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- See the reference slides for the formal definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69851a1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Formal definition of convexity\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 55%; float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d78f8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- A function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is **convex** if, for **every** $a, b$ in the domain of $f$, and for every $t \\in [0, 1]$:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\boxed{(1 - t) f(a) + t f(b) \\geq f((1-t)a + tb)}$$\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "- This is a formal way of restating the definition from the previous slide.\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 57%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"imgs/convex-definition.png\" width=100%>\n",
    "\n",
    "</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8ef7f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- The slider below depicts an interactive version of the formal definition of convexity above.<br>Drag the `a`, `b`, and `t` sliders and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf491c23",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "interact(util.convexity_visual, a=(-20, 5, 0.1), b=(5, 20, 0.1), t=FloatSlider(min=0, max=1, step=0.01, value=0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abafd124",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Second derivative test for convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb1080",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $f(w)$ is a function of a single variable and is **twice** differentiable, then $f(w)$ is convex **if and only if**:\n",
    "\n",
    "$$\\frac{d^2f}{dw^2}(w) \\geq 0, \\:\\:\\: \\forall \\: w$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6142026",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $f(w) = w^4$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f80d46",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why does convexity matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc71e86",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Convex functions are (relatively) easy to minimize with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c4ec2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Theorem**: If $f(w)$ is convex and differentiable, then gradient descent converges to a **global minimum** of $f$, as long as the step size is small enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1b022",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Why?**\n",
    "  - Gradient descent converges when the derivative is 0.\n",
    "  - For convex functions, the derivative is 0 only at one place ‚Äì the global minimum.\n",
    "  - In other words, if $f$ is convex, gradient descent won't get \"stuck\" and terminate in places that aren't global minimums (local minimums, saddle points, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e931b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonconvex functions and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6fe6e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We say a function is **nonconvex** if it does not meet the criteria for convexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ae238",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Nonconvex functions are (relatively) difficult to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9aacf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent **might** still work, but it's not guaranteed to find a global minimum.\n",
    "  - We saw this at the start of the lecture, when trying to minimize $f(w) = 5w^4 - w^3 - 5w^2 + 2w - 9$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cbc8d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing a step size in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea109e21",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, choosing a step size involves a lot of trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6784f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In this class, we've only touched on \"constant\" step sizes, i.e. where $\\alpha$ is a constant.\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80c91a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Remember**: $\\alpha$ is the \"step size\", but the amount that our guess for $w$ changes is $\\alpha \\frac{df}{dw}(w^{(t)})$, not just $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179500e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In future courses, you may learn about \"decaying\" step sizes, where the value of $\\alpha$ decreases as the number of iterations increases.<br><small>Intuition: take much bigger steps at the start, and smaller steps as you progress, as you're likely getting closer to the minimum.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149fef3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent for functions of multiple variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2c8f9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1ae0a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will typically use gradient descent to minimize empirical risk functions, $R(\\vec w)$, to find optimal model parameters.<br>A model with $d+1$ parameters $w_0, w_1, ..., w_d$ will have a $(d+2)$-dimensional loss surface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640630fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the following example function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$:\n",
    "\n",
    "$$f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f1b5ab",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.make_3D_surface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9af5b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "util.make_3D_contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12170f43",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964a882",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the function:\n",
    "\n",
    "$$f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbf0b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It has two **partial derivatives**: $\\frac{\\partial f}{\\partial w_1}$ and $\\frac{\\partial f}{\\partial w_2}$.<br><small>See the annotated slides for what they are and how we find them.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ba3c7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The gradient vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7914ff0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $f(\\vec{w})$ is a function of multiple variables, then its **gradient**, $\\nabla f (\\vec{w})$, is a vector containing its partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159e9e8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: \n",
    "\n",
    "$$f(\\vec{w}) = f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$\n",
    "\n",
    "$$\\nabla f(\\vec w) = \\begin{bmatrix} 6\\cos(2w_1)\\cos(2w_2) + 2w_1 \\\\ -6\\sin(2w_1)\\sin(2w_2) + 2w_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba622f6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example:\n",
    "\n",
    "$$f(\\vec{w}) = \\vec{w}^T \\vec{w}$$\n",
    "\n",
    "$$\\nabla f(\\vec{w}) = 2 \\vec{w}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc574518",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does the gradient vector describe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9a284",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the visual example from two slides ago.\n",
    "\n",
    "$$f(\\vec{w}) = f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$\n",
    "\n",
    "$$\\nabla f(\\vec w) = \\begin{bmatrix} 6\\cos(2w_1)\\cos(2w_2) + 2w_1 \\\\ -6\\sin(2w_1)\\sin(2w_2) + 2w_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960969a6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.make_3D_contour(with_gradient=True, w1_start=-0.2, w2_start=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5189a1a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <b><span style=\"color:gold\">gradient vector</span></b> at a point ($w_1, w_2$) describes the **direction of steepest ascent**, i.e. the direction in which the function $f$ is **increasing the quickest**, when standing at $(w_1, w_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655294b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Moving _against_ the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7edd8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <b><span style=\"color:gold\">gradient vector</span></b> at a point ($w_1, w_2$) describes the **direction of steepest ascent**, i.e. the direction in which the function $f$ is **increasing the quickest**, when standing at $(w_1, w_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37fd6f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize $f$, when starting at ($w_1, w_2$), we should move in the direction <b><span style=\"color:red\">opposite to the gradient</span></b>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428753cf",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.make_3D_contour(with_gradient=True, w1_start=-0.2, w2_start=0.5, neg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24136d86",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229467d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: \n",
    "\n",
    "$$f(\\vec{w}) = f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$\n",
    "\n",
    "$$\\nabla f(\\vec w) = \\begin{bmatrix} 6\\cos(2w_1)\\cos(2w_2) + 2w_1 \\\\ -6\\sin(2w_1)\\sin(2w_2) + 2w_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7424ccf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The global minimizer* of $f$ is a vector, $\\vec{w}^* = \\begin{bmatrix} w_1^* \\\\ w_2^* \\end{bmatrix}$.<br><small>*If one exists.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8e58f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We start with an initial guess, $\\vec{w}^{(0)}$, and step size $\\alpha$, and update our guesses using:\n",
    "\n",
    "$$\\vec{w}^{(t+1)} = \\vec{w}^{(t)} - \\alpha \\nabla f(\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd122a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdf433",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's visualize the execution of gradient descent on our trigonometric example.<br>Change `w1_start`, `w2_start`, and `step_size` below and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ee036",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_paths(w1_start=1, w2_start=-0.5, iterations=10, step_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41dcea4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Activity\n",
    "    \n",
    "Consider the following function.\n",
    "\n",
    "$$f(w_1, w_2) = (w_1-2)^2 + 2w_1 - (w_2-3)^2$$\n",
    "    \n",
    "<br>\n",
    "\n",
    "Given an initial guess of $\\vec{w}^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and a step size of $\\alpha = \\frac{1}{3}$, perform **two** iterations of gradient descent. What is $\\vec{w}^{(2)}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5bbd1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce77fec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gradient descent for simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150707a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find optimal model parameters for the model $H(x_i) = w_0 + w_1 x_i$ and squared loss, we minimized empirical risk:\n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - (w_0 + w_1 x_i ))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1d180",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is a function of multiple variables, and is differentiable, so it has a gradient!\n",
    "\n",
    "$$\\nabla R(\\vec{w}) = \\begin{bmatrix} \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i)) \\\\ \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1x_i))x_i  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8adc1bd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: To find $\\vec{w}^* = \\begin{bmatrix} w_0^* \\\\ w_1^* \\end{bmatrix}$, we _could_ use gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e15257",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why would we, when closed-form solutions exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998bc7ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for simple linear regression, visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a2031",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('oMk6sP7hrbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c06f89",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for simple linear regression, implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762a5ca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use gradient descent to fit a simple linear regression model to predict commute time in `'minutes'` from `'departure_hour'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef793e46",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df[['departure_hour', 'minutes']]\n",
    "util.make_scatter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65537ec",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = df['departure_hour']\n",
    "y = df['minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030f109",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, let's remind ourselves what $w_0^*$ and $w_1^*$ are supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4975b0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "slope = np.corrcoef(x, y)[0, 1] * np.std(y) / np.std(x)\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab40e3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "intercept = np.mean(y) - slope * np.mean(x)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f1716",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288c0f5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - (w_0 + w_1 x_i ))^2$$\n",
    "\n",
    "$$\\nabla R(\\vec{w}) = \\begin{bmatrix} \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i)) \\\\ \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1x_i))x_i  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dedbf5f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dR_w0(w0, w1):\n",
    "    return -2 * np.mean(y - (w0 + w1 * x))\n",
    "def dR_w1(w0, w1):\n",
    "    return -2 * np.mean((y - (w0 + w1 * x)) * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e68479",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d98ad",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The update rule we'll follow is:\n",
    "\n",
    "$$\\vec{w}^{(t+1)} = \\vec{w}^{(t)} - \\alpha \\nabla R(\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7d857",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can treat this as two separate update equations:\n",
    "\n",
    "$$w_0^{(t+1)} = w_0^{(t)} - \\alpha \\frac{\\partial R}{\\partial w_0} (\\vec{w}^{(t)}) \\\\ w_1^{(t+1)} = w_1^{(t)} - \\alpha \\frac{\\partial R}{\\partial w_1} (\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3a308",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's initialize $w_0^{(0)} = 100$ and $w_1^{(0)} = -50$, and choose the step size $\\alpha = 0.01$.<br><small>The initial guesses were just parameters that we thought might be close.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70a1de",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We'll store our guesses so far, so we can look at them later.\n",
    "def gradient_descent_for_regression(w0_initial, w1_initial, alpha, threshold=0.0001):\n",
    "    w0, w1 = w0_initial, w1_initial\n",
    "    w0_history = [w0]\n",
    "    w1_history = [w1]\n",
    "    while True:\n",
    "        w0 = w0 - alpha * dR_w0(w0, w1)\n",
    "        w1 = w1 - alpha * dR_w1(w0, w1)\n",
    "        w0_history.append(w0)\n",
    "        w1_history.append(w1)\n",
    "        if np.abs(w0_history[-1] - w0_history[-2]) <= threshold:\n",
    "            break\n",
    "    return w0_history, w1_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffffa0a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_history, w1_history = gradient_descent_for_regression(0, 0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4a669",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e37dc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w1_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5191",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that we converge at the right value! But how many iterations did it take? What could we do to speed it up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809aa0f2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(w0_history)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
