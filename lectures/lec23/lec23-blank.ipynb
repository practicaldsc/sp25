{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5615e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to get everything set up.\n",
    "from lec_utils import *\n",
    "import lec23_util as util\n",
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = diabetes[(diabetes['Glucose'] > 0) & (diabetes['BMI'] > 0)]\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe470a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 23\n",
    "\n",
    "# Logistic Regression, Continued\n",
    "\n",
    "### EECS 398: Practical Data Science, Spring 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/sp25\">github.com/practicaldsc/sp25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/78535/discussion/6647877) </small>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2c962",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Recap: Logistic regression.\n",
    "- Choosing a threshold.\n",
    "- Linear separability.\n",
    "- Softmax regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f71f31",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010cbf25",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Logistic regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba04a65",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051e02c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic **regression** is a linear **classification** technique that builds upon linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849627c5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It models **the probability of belonging to class 1, given a feature vector**:\n",
    "    \n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4da1ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we train a logistic regression model to predict the probability a patient has diabetes ($y = 1$) given their `'Glucose'` and `'BMI'`.<br>If our optimal parameters end up being $\\vec{w}^* = \\begin{bmatrix} -7.85 & 0.04 & 0.08 \\end{bmatrix}^T$, we then predict probabilities using:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i, \\text{BMI}_i) = \\sigma(‚àí7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67340f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find the optimal parameters $\\vec{w}^*$, we minimize mean **cross-entropy loss**:\n",
    "<br><small>There's no closed-form solution for $\\vec{w}^*$, so we use some numerical method (or, rather, `sklearn` does).</small>\n",
    "\n",
    "\\begin{align*}R_\\text{ce}(\\vec{w}) &= - \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right) \\\\ &= - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right]\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987559b7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7882d69",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To illustrate, let's re-fit a model to predict diabetes from `'Glucose'` and `'BMI'` in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b5d5b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_logistic_multiple = LogisticRegression()\n",
    "model_logistic_multiple.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b79bad",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, the `predict` method of a fit `LogisticRegression` model predicts a **class**; it applies a threshold $T = 0.5$ to the predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea1891",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple.predict(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "    'BMI': 25,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74846f8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can access the predicted **probabilities** using the `predict_proba` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd47e4b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple.predict_proba(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "    'BMI': 25,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffbadf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The decision boundary in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711af30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After choosing $T = 0.5$, what does the resulting <b><span style=\"color:purple\">decision boundary</span></b> look like, in a $d = 2$ dimensional plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cb1df",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_logistic_multiple, X_train, y_train, title='Logistic Regression Decision Boundary (T = 0.5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310b1d5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that unlike the decision boundaries for $k$-Nearest Neighbors and decision trees, this decision boundary is **linear**. Specifically, it is the line:\n",
    "\n",
    "$$\\sigma(‚àí7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i) = 0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a8cb2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: Since $\\sigma(0) = 0.5$, we can write the above as:\n",
    "\n",
    "$$-7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712fa94",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Which expression describes the **odds ratio**, $$\\frac{P(y_i = 1 | \\vec{x}_i)}{P(y_i = 0 | \\vec{x}_i)}$$\n",
    "    \n",
    "in the logistic regression model?\n",
    "    \n",
    "- A. $\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)$\n",
    "- B. $-\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)$\n",
    "- C. $e^{\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)}$\n",
    "- D. $\\sigma(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i))$\n",
    "- E. None of the above.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c41bbe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Which expression describes $P(y_i = \\mathbf{0} | \\vec{x}_i)$ in the logistic regression model?\n",
    "    \n",
    "- A. $\\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$\n",
    "- B. $-\\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$\n",
    "- C. $\\sigma\\left(- \\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$\n",
    "- D. $1 - \\log \\left( 1 + e^{\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)} \\right)$\n",
    "- E. $1 + \\log \\left( 1 + e^{- \\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)} \\right)$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac6afe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing a threshold\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df8f90",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa8052",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we've seen, in order to classify $\\vec{x}_i$ as either yes ($y_i = 1$) or no ($y_i = 0$), we apply a **threshold** $T$ to the predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a185cf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"imgs/threshold.svg\" width=600><small>With a threshold of $T = 0.6$, a predicted probability of 0.68 is classified as <span style=\"color:blue\">yes diabetes (class 1)</span>,<br>and a predicted probability of 0.55 is classified as <span style=\"color:orange\">no diabetes (class 0)</span>.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593db431",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- More generally, if we pick a threshold of $T$, then any feature vector $\\vec{x}_i$ such that:\n",
    "\n",
    "    $$\\sigma(\\vec{w}^* \\cdot \\text{Aug}(\\vec{x}_i)) \\geq T$$ \n",
    "\n",
    "    is classified as class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c8035",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we choose the \"right\" threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df34090",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn`'s default threshold of $T = 0.5$ is **not** guaranteed to yield the highest **accuracy**!<br><small>Remember, to find $\\vec{w}^*$, we minimized mean cross-entropy loss (that is, we didn't \"maximize\" accuracy), and mean cross-entropy loss doesn't involve our threshold.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66b4ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing a custom threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b928d4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we want to use a custom threshold, we'll need to implement the logic ourselves.\n",
    "\n",
    "<center><img src=\"imgs/threshold.svg\" width=300></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1e750",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_thresholded(X, T):\n",
    "    '''Calls model_logistic_multiple.predict_proba.\n",
    "       For each P(y_i = 1 | x_i), returns 1 if >= T and 0 if < T.'''\n",
    "    probs = model_logistic_multiple.predict_proba(X)[:, 1]\n",
    "    return (probs >= T).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c10e87",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, we can choose any threshold we'd like, and compute the accuracy of the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795dbb95",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_thresholded([[150, 25]], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf574aba",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_thresholded([[150, 25]], 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a146c19",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_thresholded(X_train, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989837a0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training accuracy for the threshold T = 0.4.\n",
    "(predict_thresholded(X_train, 0.4) == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316991a2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60420c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy is defined as:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{# points classified correctly}}{\\text{# points}} = \\frac{TP + TN}{TP + FP + FN + TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620688eb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's **training** accuracy change as the threshold changes?<br><small>Note that we'd see a similar trend with test accuracy, too.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202ef9e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train, y_train, 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921035f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The threshold with the best training accuracy (among the thresholds we tried) is $T = 0.465$, which has a training accuracy of 77.3\\%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4abc7d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember that 64\\% of people in the training set don't have diabetes, so we can achieve a 64\\% training accuracy just by always predicting \"no diabetes\"! This means that a good model's accuracy should be much higher than 64\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0145f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67f87a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metrics for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e8111",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A few lectures ago, we introduced other metrics for measuring the quality of a binary classifier's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7369a46",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "<center><small>Here, a false positive ($FP$) is when we predict that someone has diabetes when they do not.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77423feb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "<center><small>Here, a false negative ($FN$) is when we predict that someone does not have diabetes, when they really do.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66609e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A binary classifier's **confusion matrix** displays its number of true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfa9ac",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_confusion(X_train, y_train, T=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53f80f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, we're predicting whether or not patients have diabetes. **Which is worse: a false positive or a false negative?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4ebe7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe how the values in the confusion matrix change as the threshold changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8fef2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(lambda T: util.show_confusion(X_train, y_train, T), T=(0, 1, 0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cc745",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34813ef4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Precision is defined as:\n",
    "\n",
    "    $$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$\n",
    "    \n",
    "    Here, a false positive ($FP$) is when we predict that someone has diabetes when they do not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbc58b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's training **precision** change as the threshold changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0063a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train, y_train, 'Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc04e6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the \"bar\" is higher to predict 1, then we will have fewer positives in general, and thus fewer false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c054b96",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As the **threshold increases** ‚¨ÜÔ∏è, the denominator in $\\text{precision} = \\frac{TP}{TP + FP}$ will decrease, and so **precision tends to increase** ‚¨ÜÔ∏è.<br><small>There are some cases where a slightly higher threshold led to a slightly lower precision; why?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe599fa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff0195",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall is defined as:\n",
    "\n",
    "    $$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$\n",
    "    \n",
    "    Here, a false negative ($FN$) is when we predict that someone does not have diabetes, when they really do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25fd88",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's training **recall** change as the threshold changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa4692",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train, y_train, 'Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f054603",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the denominator in $\\text{recall} = \\frac{TP}{\\text{# actually positive}}$ is constant. As the **threshold increases** ‚¨ÜÔ∏è:\n",
    "    - true positives get converted to false negatives, so\n",
    "    - the numerator of recall ($TP$) decreases, and so\n",
    "    - **recall decreases** ‚¨áÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a3226",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision vs. recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b02705",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can visualize how precision and recall vary **together**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152982c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.pr_curve(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006b6d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The curve above is called a **PR curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42a46c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: Given the information above, what threshold would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb5382",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Answer**: The threshold whose point is closest to the **top right corner** of the plot above. <br><small>Why? The top right corner is where precision = 1 and recall = 1, and we want both to be high.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449396a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec567abd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A more popular variant of the PR curve is the **ROC curve**.<br><small>ROC stands for \"receiver operating characteristic.\"<br>See [**here**](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves) for a good discussion on the differences between PR curves and ROC curves.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8404c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A ROC curve plots true positive rate (TPR) vs. false positive rate (FPR) for all possible thresholds, where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312f23c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{\\text{true positive rate (TPR)} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN} = \\text{recall}}_\\text{we want this to be close to 1!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d1ad8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{\\text{false positive rate (FPR)} = \\frac{FP}{\\text{# actually negative}} = \\frac{FP}{FP + TN}}_\\text{we want this to be close to 0!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655fe8c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The ROC curve for our classifier looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc20e4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.draw_roc_curve(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285487d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we care about TPR and FPR equally, the best threshold is the one whose point is closest to the **top left corner** in the plot above.<br><small>Why? The top left corner is where $TPR = 1$ and $FPR = 0$, and we want $TPR$ to be high and $FPR$ to be low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a477e2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A common metric for the quality of a binary classifier is the **area under curve (AUC)** for the ROC curve.<br><small>Larger values are better!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5bc9b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about thresholds and logistic regression?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe948d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear separability\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4fcf5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef082d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're using $d$ features as inputs to our classifier. Consider a visualization of the features in $d$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21523cdb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $d = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3347c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_in_1D(X_train, y_train, thres=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabfb62",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $d = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8db0c6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "util.create_base_scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17621f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that in both plots above, there are <span style=\"color:orange\">orange points</span> mixed in with the <span style=\"color:blue\">blue points</span>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c9cf1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear separability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d94a38",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A dataset is **linearly separable** if a line, plane, or hyperplane can be drawn in $d$-dimensional space that **perfectly separates** the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d513b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $d = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffe646",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef65c0dc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.non_lin_sep_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac141198",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Example: $d = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606a3ed",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda4307",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.non_lin_sep_2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77870586",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Why is the dataset below **not** linearly separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03038c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "util.bad_example_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ed89e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear separability and decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd2f70",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By definition, if a dataset is linearly separable, then there exists a **<span style=\"color:purple\">linear decision boundary</span>** that achieves 100\\% training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c435423",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d38ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Above, any value of $c$ in $(120, 150)$ would make the <b><span style=\"color:purple\">decision boundary</span></b> $$\\text{Glucose} = c$$\n",
    "achieve 100% training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e7558",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we find this decision boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88d210",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression and linear separability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a088cd1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic regression, **without regularization**, **fails to converge** on linearly separable data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c3918",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's re-draw the plot below, but with diabetes status drawn on the $y$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ce60f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329f729",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why would the optimal $w_1^*$ below tend to $\\infty$?<br><small>See the annotated slides for more details.</small>\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma(w_0 + w_1 \\cdot \\text{Glucose}_i) = \\frac{1}{1 + e^{-(w_0 + w_1 \\cdot \\text{Glucose}_i)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35af3c7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_1D_elevated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8bf1f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To prevent this case, logistic regression should generally be regularized.<br><small>This is exactly why `sklearn` regularizes logistic regression by default.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb724b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic regression for multiclass classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ed46d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From binary to multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a53918",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In binary classification, there are only two possible classes, typically either 0 or 1.\n",
    "\n",
    "$$y_i \\in \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f8fce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In multiclass classification, there can be any finite number of classes, or **labels**. They need not be numbers, either.\n",
    "\n",
    "$$y_i \\in \\{ \\text{Adelie}, \\text{Chinstrap}, \\text{Gentoo} \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac384f44",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: Let $C$ be the set of possible classes for our classification problem, and let $|C|$ be the number of classes total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9cc3e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the data üêß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f2096",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "penguins = sns.load_dataset('penguins').dropna().reset_index(drop=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(penguins[['bill_length_mm', 'body_mass_g']], \n",
    "                                                    penguins['species'], \n",
    "                                                    random_state=26)\n",
    "display(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e768f04",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we did two lectures ago, we'll aim to predict the `'species'` of a penguin given their `'bill_length_mm'` and `'bill_depth_mm'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68968185",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.penguin_scatter_2d(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9fcaf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f464f7d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a $k$-NN classifier with $k=5$ to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ec0e5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn.fit(X_train, y_train)\n",
    "util.penguin_decision_boundary(model_knn, X_train, y_train, title=\"k-NN Decision Boundary when k = 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c9516",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice the vastly different scales of the features! What happens if we standardize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0fe9c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "model_knn_standardized = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5))\n",
    "model_knn_standardized.fit(X_train, y_train)\n",
    "util.penguin_decision_boundary(model_knn_standardized, X_train, y_train, title=\"k-NN Decision Boundary when k = 5 and with Standardization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b0af6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ac5bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a decision tree classifier with a maximum depth of 3 to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028f58c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_tree = DecisionTreeClassifier(max_depth=3)\n",
    "model_tree.fit(X_train, y_train)\n",
    "util.penguin_decision_boundary(model_tree, X_train, y_train, title=\"Decision Boundary for a Decision Tree of Depth 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8559cc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3fb4fe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we've seen, in **binary classification**, logistic regression models **the probability of belonging to class 1, given a feature vector $\\vec{x}_i$**:\n",
    "\n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4405c3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In logistic regression, $C = \\{0, 1\\}$. But, in our current penguin classification problem, $C = \\{ \\text{Adelie}, \\text{Chinstrap}, \\text{Gentoo} \\}$, so we can't use logistic regression directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dec5cb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One idea: **one-vs-rest**. Fit $|C| = 3$ separate logistic regression models ‚Äì one per class ‚Äì and predict the class that has the highest probability.\n",
    "    - Penguin is Adelie vs. penguin is not Adelie.\n",
    "    - Penguin is Chinstrap vs. penguin is not Chinstrap.\n",
    "    - Penguin is Gentoo vs. penguin is not Gentoo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa267f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another idea: **one-vs-one**. Fit ${3 \\choose 2} = 3$ separate logistic regression models ‚Äì one per **pair** of classes ‚Äì and predict the class that \"wins\" the most predictions.\n",
    "    - Penguin is Adelie vs. penguin is Chinstrap.\n",
    "    - Penguin is Adelie vs. penguin is Gentoo.\n",
    "    - Penguin is Chinstrap vs. penguin is Gentoo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894c514",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try something slightly different than what's listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5dd92",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f3f82",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Multinomial** logistic regression, also known as **softmax regression**, models the probability of belonging to **any class, given a feature vector $\\vec x_i$**.<br><small>Think of it as a generalization of logistic regression.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfe803",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p_\\text{Adelie} = P(y_i = \\text{Adelie} | \\vec{x}_i) = \\frac{e^{\\vec{w}_\\text{Adelie} \\cdot \\text{Aug}(\\vec{x}_i)}}{e^{\\vec{w}_\\text{Adelie} \\cdot \\text{Aug}(\\vec{x}_i)} + e^{\\vec{w}_\\text{Chinstrap} \\cdot \\text{Aug}(\\vec{x}_i)} + e^{\\vec{w}_\\text{Gentoo} \\cdot \\text{Aug}(\\vec{x}_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb30d60",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p_\\text{Chinstrap} = P(y_i = \\text{Chinstrap} | \\vec{x}_i) = \\frac{e^{\\vec{w}_\\text{Chinstrap} \\cdot \\text{Aug}(\\vec{x}_i)}}{e^{\\vec{w}_\\text{Adelie} \\cdot \\text{Aug}(\\vec{x}_i)} + e^{\\vec{w}_\\text{Chinstrap} \\cdot \\text{Aug}(\\vec{x}_i)} + e^{\\vec{w}_\\text{Gentoo} \\cdot \\text{Aug}(\\vec{x}_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd8733",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{p_j = P(y_i = j | \\vec{x}_i) = \\frac{e^{\\vec{w}_j \\cdot \\text{Aug}(\\vec{x}_i)}}{\\sum_{k \\in C} e^{\\vec w_k \\cdot \\text{Aug}(\\vec x_i)}}}_\\text{in general}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684951b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of a single parameter vector $\\vec{w}$, there are $|C|$ parameter vectors, one per class!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cc516",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multinomial logistic regression models the probability of each class directly, and then predicts the most likely class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738095b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: The softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fbdfb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **softmax** function is a generalization of the logistic function to multiple dimensions.<br>\n",
    "Suppose $\\vec z \\in \\mathbb{R}^d$. Then, the softmax of $\\vec z$ is defined element-wise as follows:\n",
    "\n",
    "$$\\sigma(\\vec z)_i = \\frac{e^{z_i}}{\\sum_{j = 1}^d e^{z_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f4497",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, suppose $\\vec{z} = \\begin{bmatrix} -5 \\\\ 2 \\\\ 4 \\end{bmatrix}$. Then:\n",
    "\n",
    "$$\\sigma(\\vec z) = \\begin{bmatrix} \\sigma(\\vec z)_1 \\\\ \\sigma(\\vec z)_2 \\\\ \\sigma(\\vec z)_3  \\end{bmatrix} = \\underbrace{\\begin{bmatrix} \\frac{e^{-5}}{e^{-5} + e^2 + e^4} \\\\ \\frac{e^{2}}{e^{-5} + e^2 + e^4} \\\\ \\frac{e^{4}}{e^{-5} + e^2 + e^4} \\end{bmatrix}}_\\text{note the constant denominator!} = \\begin{bmatrix} 0.0001 \\\\ 0.1192 \\\\ 0.8807 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8e687",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why is it defined this way? **It maps a vector of real numbers to a vector of probabilities!**<br><small>Note that the denominator, $\\sum_{j=1}^d e^{z_j}$, normalizes the $e^{z_i}$ terms so that the results sum to 1.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695cc93",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multinomial logistic regression, i.e. softmax regression, trains $|C|$ linear models of the form $\\boxed{\\vec w_k \\cdot \\text{Aug}(\\vec x_i)}$ ‚Äì one per class, $k$ ‚Äì and feeds the output of each through the softmax function, so the results can be interpreted as probabilities.\n",
    "\n",
    "    $$p_j = P(y_i = j | \\vec{x}_i) = \\frac{e^{\\vec{w}_j \\cdot \\text{Aug}(\\vec{x}_i)}}{\\sum_{k \\in C} e^{\\vec w_k \\cdot \\text{Aug}(\\vec x_i)}}$$\n",
    "\n",
    "    The $|C|$ optimal parameter vectors ‚Äì $\\vec w_\\text{Adelie}^*$, $\\vec w_\\text{Chinstrap}^*$, and $\\vec w_\\text{Gentoo}^*$, in our case ‚Äì are chosen to minimize mean cross-entropy loss, just like before!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af2e92",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multinomial logistic regression in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c586fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `LogisticRegression` class supports multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ba7ab",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log = LogisticRegression(multi_class='multinomial')\n",
    "model_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0851421",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In total, the fit model has $3 \\times 2 = 6$ coefficients and $3 \\times 1 = 3$ intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39041d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628a273",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6759cca",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79f8c5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When calling `model_log.predict_proba`, we get back an array of three predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b1393",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.predict_proba(pd.DataFrame([{\n",
    "    'bill_length_mm': 45,\n",
    "    'body_mass_g': 4500\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c91ba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does this model _look_ like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09b48b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.penguin_decision_boundary(model_log, X_train, y_train, title=\"Softmax Regression Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261ee61",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural networks üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b173c3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Softmax regression is an example of a **neural network**.<br><small>Our brains are made up of **neurons** connected by \"links\", called synapses. The model diagram below loosely resembles this structure, which is why the model is called a **neural** network.</small>\n",
    "\n",
    "<center><img src=\"imgs/net.svg\" width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ebdc24",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each of the 9 diagonal lines connecting a value in the <b><span style=\"color:#b3e0ff\">input layer</span></b> with a value in the <b><span style=\"color:#ff7400\">out</span><span style=\"color:#c45bcc\">put</span> <span style=\"color:#077575\">layer</span></b> represents a parameter, $w^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1db7d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b86b20",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afad7b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can use the nine parameter values above to reproduce the network's calculations ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359066a3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Same values as shown in model_log.predict, two slides ago!\n",
    "softmax = lambda z: np.e ** z / sum(np.e ** z)\n",
    "softmax(model_log.intercept_.reshape(-1, 1) + model_log.coef_ @ np.array([[45], [4500]]))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
